
Theory 5: Networking Concepts for Effective Backend Applications (Selected Topics)


Listening to Servers: 

---> A logical server can be created to listen on a specific socket (IP address + Port number). 

---> A machine normally has multiple interfaces with multiple IP addresses (public + private, IPv4 + IPv6). 

---> The socket that a logical server (e.g. a web server) listens to is criticial for safety: 

	---> E.g. (127.0.0.1, 8080) => This socket specifies an IPv4 interface on port 8080, which is a specific socket to listen to. 

	---> (::1, 8080) => This is socket is an IPv6 interface on port 8080

	---> (0.0.0.0, 8080) => This combination instructs to listen to all possible interfaces / IP addresses on port 8080. 

	---> From the above examples, listening to a specific socket as opposed to all possible sockets with port 8080 is a safe option. 

	---> The 3rd example is dangerous because this opens the server to all possible connection attempts, which can open the server to attacks and other dangerous parties. 


---> Important: Only one process can listen to one socket at a time, except in the case of port reuse in operating systems. 

	---> Port reuse is when an OS balances the connections associated with different processes to a single port number. 

	---> The OS creates a hash of the file descriptor for a connection in order to implement, by matching communications from different processes and load balancing them to the same port. 



The Importance of Proxies and Reverse Proxes: 

---> A proxy is a machine which makes a request to a server on the behalf of a client.  

	---> When a client makes a request to a particular website, it is likely that the actual request is made to a proxy (ie a "forward proxy" / client facing proxy) and the proxy makes the request for the client. 

	---> The client actually makes a TCP connection establishment with the proxy and the proxy makes a TCP connection with the actual server in order to forward the request to the server. 

	---> On layer 4, the client only sees the proxy socket and not the server socket to which it makes a connection to, but on layer 7 the application code may have HTTP headers which contain information relating to the client's original IP address. 

		---> An example of this is the HTTP header called X-Forwarded-For (XFF) which is the de-facto standard header used to identify the originating IP address of a client connecting to a web server via a proxy server. 

	
	---> In essence, the client will know the final destination (e.g. google.com) even though the TCP connection is made to the (forward) proxy.  
		
		---> Important: The proxy is usually located in the same network as the client machine, which is why it is also called a "forward proxy".



---> A proxy can have many different use cases.  The main use cases are: 

	A. Caching => A proxy can be used to cache frequently requested resources in order to help increase performance across the entire system. 

	B. Anonymity => A proxy can also be used to intercept requests and hide the source IP addresses of the request. 

	C. Logging => Proxies can have all service communications (e.g. in a mesh topology) going through it and the details + events surrounding these communications can be centrally logged. 

		---> A tool like APIGEE could be considered a primary example of this use case. 


	D. Blocking sites / traffics => Proxies can block traffic directed to / from certain sites in different ways (see fire wall notes in non-techies course).  Thus they act as firewalls in cyber security strategies. 



---> A reverse proxy essentially acts as the reverse to the proxy.  The reverse proxy resides typically in the same network as a server.

	---> Here, the client traffic is intercepted before the traffic / request reaches the actual servers. 

	---> The reverse proxy delegates the traffic to their proper destination servers and the servers use the reverse proxy to achieve purposes relating to things such as load balancing and availability. 

	---> In this scenario, the proxy does not act on behalf of the client, but rather for the servers.  

	---> The reverse proxy is essentially the "forward proxy" for servers in that they process incoming traffic to servers and delegate that traffic to the server all the while ensuring that the true destination IP address of the server is never known to the client.  

	---> The concept of socket connections is also considered from the perspective of reverse proxies: 

		---> The destination server makes a TCP connection with the reverse proxy and thus knows the reverse proxy socket on a layer 4 perspective. 

		---> The reverse proxy is what makes the TCP connection with the client socket. 

		---> However, for security reasons, there is usually no IP forwarding of the destination server IP to the client (unlike the cases for a forward proxy). 

	
	---> The complete picture is that the direction of the traffic must be considered with respect to the proxy + client / server in order to properly understand the roles each type of proxy server plays. 

	---> Reverse proxies are primarily used for caching, load balancing, security (e.g. firewalls), and canary deployments (?) among other use cases. 

	---> Examples of reverse proxies: NGINX, apache, etc.


---> It can be that a forward proxy contacts a reverse proxy on behalf of the client and this can be a scenario where both types of proxies are used at the same time. 

	---> Important: A reverse proxy can never be a forward proxy. 


---> On average, proxies are commonly used for HTTP network traffic. 



Access Control to Database Servers:

---> This topic refers to considering the types of connections that can be made to a database. 

---> For example, a PostGreSQL database can sport some of the following types of connection configurations: 

	A. Default connection => This is a direct connection, from the app / "client" to the database.  Ie the consumer of the database connects to the process the database is running on directly, with no involvement from a TCP connection. 

		---> Performance wise this is the fastest type of communication. 


	B. Loopback address connection => This is a connection in which the loopback address of the machine is used, with the TCP stack (ie layer 4), in order to form a connection to the database. 

		---> This can be applied for IPv4 and IPv6 variations. 

		---> This type of connection only allows for connection of an app to a database on the same machine only, but with interaction from TCP. 

			---> Therefore, this is less performant, but easier to implement than the direct process connection mentioned above. 

		
		---> In IPv4, the subnet mask is used in 2 different ways (ie via CIDR notation or with direct IP subnet mask) in order to confirm the loopback address as being in the same network and thus machine. 


	C. Localhost connection => This connection is effectively the same as the loopback address connection, but the connection string of "localhost" can resolve to either IPv4 or IPv6.  


	D. LAN connection => This connection is based on an app / consumer of the DB being in the same LAN as the database and therefore possessing the same network portion of the IP address as the IP address permitted by the database. 

		---> The method to allow the connection to takeplace can be based on password authentication on a certain domain or LAN or based on identity (e.g. by OS username). 

		---> This involves forming a TCP connection between the 2 hosts within the same network using the typical subnet mask procedure to identify if the request is meant for the same network, etc. 


	E. Non-LAN connection => The network bits of the allowed IP address can be configured to allow hosts from outside of the LAN to access the database as well. 

	
	---> In all of the above examples, the attempt to form a connection is blocked first by an authentication method (e.g. based on identity, password, IP filtering, etc) before the TCP connection is allowed to form between the database socket and the host socket. 

 
---> Overall, it is important to ensure that the types of connections allowed to form with the database are considered for security purposes.  



Understanding the Cost of TCP Connections: 

---> The establishment of a TCP connection can be costly and the following factors govern the performance behind the 3-way handshake process: 

	A. Distance between hosts => Higher distance will lead to more hops over routers for IP packets / segments, thereby increasing latency. 

	B. TCP Slow Start => Due to its exponential nature, the TCP slow start algorithm starts out slowly, thereby limiting the amount of packets that can be sent out via window size (especially the handshake packets). 

	C. Congestion + Flow Control => Both of these traffic limiting mechanisms further add to the performance cost (ie latency) behind the 3-way handshake by limiting the amount of handshake segments / packets that can be sent. 

	D. Delayed Acknowledgment + Nagel's Algorithm => (To be described later) Both are responsible for latency increases for the TCP handshake. 

	---> Many of these factors also affect the 4-way termination handshake for the TCP connection destruction. 


---> To offset the cost of connection establishment, a technique called connection pooling is used: 

	---> Connection tooling is when a set number of TCP connections on a backend / server contet are established and continously maintained.  

		---> Due to the slow start algorithm (congestion control), initial requests sent over the newly established connections are slow. 

		---> However, the exponential nature of the algorithm ensures that later requests will be sent in a much faster fashion because the congestion window has increased exponentially over the intermediary routers. 


	---> Requests are queued up and sent over available TCP connections (either synchronously or asynchronously) via the use of threads (1 thread per request / response transaction).  

		---> The connections are maintained and thus avoiding continued costs of connection establishment. 

		---> Connections are only destroyed when there is no need for the connection or when the process shuts down. 


---> Finally, there is the conceptual choice between eager vs lazy loading of resources within the app context: 


	A. Eager Loading => This is the idea of loading all resources at once for use. 

		---> Requests are served immediately. 

		---> However, due to the slow start algorithm, the amount of data sent over the network is limited by small congestion windows. 

		---> Therefore, servicing requests is slow in eager loading because more data is sent back per time unit over a network connection. 

		---> The limit of the congestion windows prevent data sent back over in sufficiently large amounts over an eager loading situation. 


	B. Lazy loading => This concept is the idea of loading resources only when required explicitly by the request. 

		---> Request serving is slower under this paradigm. 

		---> Smaller volumes of data are sent over a connection per unit time, which contributes to a comparatively fast start up of lazy loading vs eager loading. 

		---> However, due to the initially incomplete sending of data, the congestion window will increase at a slower rate and therefore servicing of initial requests can be quite slow. 

			---> Vs Eager Loading => Lazy loading allows for requests to be served quickly because only the necessary parts of the request resources are given, whereas in eager loading all resources are given. 

				---> Therefore, lazy loading entails a slower rate of increase for congestion window size than eager loading.  The volume of data to service a request in eager loading is larger than lazy loading, so eager loading's initial request servicing efficiency is quite slow vs lazy loading. 


Load Balancing at Layer 4 vs Layer 7: 

---> A load balancer is a reverse proxy that is used to distribute traffic across different backend services in order to "balance" inbound traffic across a system for performance + scalability reasons. 

	---> Generically, a load balancer will send a request to one or more backends depending on several performance + load based metrics. 

	---> The load balancer can operate either on layer 4 or layer 7. 


---> A layer 4 load balancer operates on a 1 connection to 1 connection basis between client and backend destination server. 

	---> It is common for a load balancer to maintain a set of maintained connections with a set of backend servers (ie via connection pool?). 

	---> When a client forms a particular TCP connection with the layer 4 load balancer, the layer 4 load balancer will assign a correspondence with a particular connection to a destination backend server. 

	---> Therefore, there is always a 1-1 correspondence between these connections such that any traffic from the client-load balancer connection will always be assigned to that particular load balancer - destination server connection. 

	---> The segments are rewritten into new sequences (and sometimes segment sizes are adjusted) when being sent over the second connection. 

	---> If the client forms another distinct connection with the layer 4 load balancer, then a new correspondence is setup with a new backend connection. 

		---> Load balancers use specific load balancing algorithms in order to decide how to achieve this assignment (e.g. via round robin). 


---> A layer 4 load balancer has the following pros and cons:

	---> The main pro of a layer 4 load balancer is that it offers a simpler load balancing concept than a layer 7 load balancer: 

		---> This includes a lack of data lookup (ie data in segments are not read). 

		---> As an extension of the above, there is also a lack of decrypting the data in segments, allowing for layer 4 load balancing to be more secure. 

		---> Because the load balancer works for layer 4 in this case, many application layer protocols are supported (HTTP, SQL connections, gRPC, etc). 

		---> And if using NAT layer 4 load balancing (where the load balancer acts as an effective default gateway between client and server), then only 1 TCP connection is needed. 


	---> However, the pros of a layer 4 load balancer also act as its main cons in another sense:

		---> The lack of data lookup prevents contextualization of data for load balancing purposes and leads to a less "intelligent" load balancing vs layer 7 load balancers. 

		---> Layer 4 load balancers also do not offer caching for incoming traffic. 

		---> Layer 4 load balancers are protocol unaware (since many higher level protocols are not considered at layer 4) and this may allow certain traffic types to bypass normal security filtering rules. 



---> A layer 7 load balancer offers a more intelligent load balancing concept based on the fact that the layer 7 load balancer provides data lookup into the data and is protocol specific. 

	---> Data in a request is decrypted and buffered into the load balancer, which then parses the data to understand it and repackages it as a new request to its proper backend server (based on configured metrics). 

	---> This approach offers a more selective way to load balance traffic between different backend servers. 

	---> Since layer 7 load balancing is protocol specific, it is possible to send different requests, on the same connection, to different backend servers. 

	---> This is in direct contrast to a layer 4 load balancer in which a private 1-1 connection is forged between client-load balancer-server. 

	---> In the layer 4 case, the connection is stateful and that state is used to ensure that traffic always follows that connection. 

	---> In the layer 7 case, the conneciton is not stateful in some cases (e.g. stateless protocols like HTTP).  


---> For a layer 7 load balancer, the pros lie in its ability to provide smart load balancing vs layer 4 load balancers. 

	---> This includes being able to decrypt the data, understand it based on certain metrics, and also to understand the protocol. 

	---> The layer 7 load balancer can also allow for caching. 

	---> A layer 7 load balancer can also provide authentication capabilities and can act as an API gateway for all requests.  

	---> Altogether, this makes a layer 7 load balancer ideal for distributed systems composed of microservices. 

	
---> However, a layer 7 load balancer's smart load balancing approach also offers disadvantages: 

	---> Data lookup, contextualization, and decryption (TLS termination) can be quite expensive. 

	---> A minimum of 2 TCP connections must be used, unlike the case of NAT layer 4 load balancing where 1 TCP connection is used. 

	---> The load balancer and the client must share a TLS certificate, which can be undesirable in some cases. 

	---> Finally, the needs to buffer requests and to understand protocols can also be a performance bottleneck in times of high request traffic. 


---> An example of a layer 7 load balancer is NGINX. 
 


